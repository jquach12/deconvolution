{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = torch.nn.Sequential(\n",
    "            \n",
    "            #conv1\n",
    "            nn.Conv2d(1, 10, kernel_size=5),   \n",
    "            nn.ReLU(),                         #F.ReLU when given input already, nn.ReLU is idle (curry?)\n",
    "            nn.MaxPool2d(2,stride = 2, return_indices = True),        \n",
    "            \n",
    "            #conv2\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            #nn.Dropout2d()\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,stride = 2, return_indices = True))\n",
    "            \n",
    "#             #fully connected layers\n",
    "#             nn.Linear(320, 50),\n",
    "#             nn.Linear(50, 10))\n",
    "        \n",
    "        self.feature_outputs = [0]*len(self.features)\n",
    "        self.pool_indices = dict()\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(320, 50),  # 224x244 image pooled down to 7x7 from features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(50, 10))\n",
    "        \n",
    "        \n",
    "        def forward_features(self,x):\n",
    "            output = x\n",
    "            for i,layer in enumerate(self.features):\n",
    "                if isinstance(layer, nn.MaxPool2d):\n",
    "                    print(\"im a maxpool!\")\n",
    "                    output, indices = layer(output)\n",
    "                    self.features_output[i] = output\n",
    "                    self.pool_indices[i] = indices\n",
    "                else:\n",
    "                    output = layer(output)\n",
    "                    self.features_output[i] = output\n",
    "            return output\n",
    "                    \n",
    "                \n",
    "\n",
    "        def forward(self, x):\n",
    "            output = self.forward_features(x)\n",
    "            output = output.view(output.size()[0], -1)\n",
    "            output = self.classifier(output)\n",
    "            return output\n",
    "            \n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN,self).__init__()\n",
    "\n",
    "        self.deconv_features = torch.nn.Sequential(\n",
    "            nn.MaxUnpool2d(2, stride=2),\n",
    "            nn.ConvTranspose2d(20, 10, kernel_size=5, padding=1),\n",
    "            nn.MaxUnpool2d(2, stride=2),\n",
    "            nn.ConvTranspose2d(10, 1, kernel_size=5, padding=1))\n",
    "\n",
    "        # not the most elegant, given that I don't need the MaxUnpools here\n",
    "        self.deconv_first_layers = torch.nn.ModuleList([\n",
    "            torch.nn.MaxUnpool2d(2, stride=2),\n",
    "            torch.nn.ConvTranspose2d(1, 10, 3, padding=1),\n",
    "            torch.nn.MaxUnpool2d(2, stride=2),\n",
    "            torch.nn.ConvTranspose2d(1, 1, 3, padding=1) ])\n",
    "\n",
    "\n",
    "    def forward(self, x, layer_number, map_number, pool_indices):\n",
    "        start_idx = self.conv2DeconvIdx[layer_number]\n",
    "        if not isinstance(self.deconv_first_layers[start_idx], torch.nn.ConvTranspose2d):\n",
    "            raise ValueError('Layer '+str(layer_number)+' is not of type Conv2d')\n",
    "        # set weight and bias\n",
    "        self.deconv_first_layers[start_idx].weight.data = self.deconv_features[start_idx].weight[map_number].data[None, :, :, :]\n",
    "        self.deconv_first_layers[start_idx].bias.data = self.deconv_features[start_idx].bias.data        \n",
    "        # first layer will be single channeled, since we're picking a particular filter\n",
    "        output = self.deconv_first_layers[start_idx](x)\n",
    "\n",
    "        # transpose conv through the rest of the network\n",
    "        for i in range(start_idx+1, len(self.deconv_features)):\n",
    "            if isinstance(self.deconv_features[i], torch.nn.MaxUnpool2d):\n",
    "                output = self.deconv_features[i](output, pool_indices[self.unpool2PoolIdx[i]])\n",
    "            else:\n",
    "                output = self.deconv_features[i](output)\n",
    "        return output\n",
    "                    \n",
    "            \n",
    "      \n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, ceil\n",
    "import numpy as np\n",
    "\n",
    "def visualize_grid(Xs, ubound=255.0, padding=1):\n",
    "  \"\"\"\n",
    "  Reshape a 4D tensor of image data to a grid for easy visualization.\n",
    "  Inputs:\n",
    "  - Xs: Data of shape (N, H, W, C)\n",
    "  - ubound: Output grid will have values scaled to the range [0, ubound]\n",
    "  - padding: The number of blank pixels between elements of the grid\n",
    "  \"\"\"\n",
    "  (N, H, W, C) = Xs.shape\n",
    "  grid_size = int(ceil(sqrt(N)))\n",
    "  grid_height = H * grid_size + padding * (grid_size - 1)\n",
    "  grid_width = W * grid_size + padding * (grid_size - 1)\n",
    "  grid = np.zeros((grid_height, grid_width, C))\n",
    "  next_idx = 0\n",
    "  y0, y1 = 0, H\n",
    "  for y in range(grid_size):\n",
    "    x0, x1 = 0, W\n",
    "    for x in range(grid_size):\n",
    "      if next_idx < N:\n",
    "        img = Xs[next_idx]\n",
    "        low, high = np.min(img), np.max(img)\n",
    "        grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n",
    "        # grid[y0:y1, x0:x1] = Xs[next_idx]\n",
    "        next_idx += 1\n",
    "      x0 += W + padding\n",
    "      x1 += W + padding\n",
    "    y0 += H + padding\n",
    "    y1 += H + padding\n",
    "  # grid_max = np.max(grid)\n",
    "  # grid_min = np.min(grid)\n",
    "  # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n",
    "  return grid\n",
    "\n",
    "def vis_grid(Xs):\n",
    "  \"\"\" visualize a grid of images \"\"\"\n",
    "  (N, H, W, C) = Xs.shape\n",
    "  A = int(ceil(sqrt(N)))\n",
    "  G = np.ones((A*H+A, A*W+A, C), Xs.dtype)\n",
    "  G *= np.min(Xs)\n",
    "  n = 0\n",
    "  for y in range(A):\n",
    "    for x in range(A):\n",
    "      if n < N:\n",
    "        G[y*H+y:(y+1)*H+y, x*W+x:(x+1)*W+x, :] = Xs[n,:,:,:]\n",
    "        n += 1\n",
    "  # normalize to [0,1]\n",
    "  maxg = G.max()\n",
    "  ming = G.min()\n",
    "  G = (G - ming)/(maxg-ming)\n",
    "  return G\n",
    "  \n",
    "def vis_nn(rows):\n",
    "  \"\"\" visualize array of arrays of images \"\"\"\n",
    "  N = len(rows)\n",
    "  D = len(rows[0])\n",
    "  H,W,C = rows[0][0].shape\n",
    "  Xs = rows[0][0]\n",
    "  G = np.ones((N*H+N, D*W+D, C), Xs.dtype)\n",
    "  for y in range(N):\n",
    "    for x in range(D):\n",
    "      G[y*H+y:(y+1)*H+y, x*W+x:(x+1)*W+x, :] = rows[y][x]\n",
    "  # normalize to [0,1]\n",
    "  maxg = G.max()\n",
    "  ming = G.min()\n",
    "  G = (G - ming)/(maxg-ming)\n",
    "  return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-1ed01080dea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mimg_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mconv_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'VGG16 model:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg16_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def vis_layer(activ_map):\n",
    "    plt.clf()\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(activ_map[:,:,0], cmap='gray')\n",
    "\n",
    "def decon_img(layer_output):\n",
    "    raw_img = layer_output.data.numpy()[0].transpose(1,2,0)\n",
    "    img = (raw_img-raw_img.min())/(raw_img.max()-raw_img.min())*255\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     if len(sys.argv) < 2:\n",
    "#         print('Usage: '+sys.argv[0]+' img_file')\n",
    "#         sys.exit(0)\n",
    "\n",
    "    img_filename = '6.png'\n",
    "\n",
    "    n_classes = 1000 # using ImageNet pretrained weights\n",
    "\n",
    "    vgg16_c = Net()\n",
    "    conv_layer_indices = [0,3]\n",
    "\n",
    "    img = np.asarray(Image.open(img_filename).resize((28,28)))\n",
    "    \n",
    "    \n",
    "    img_var = torch.autograd.Variable(torch.FloatTensor(img.transpose(1,0)[np.newaxis,:,:].astype(float)))\n",
    "\n",
    "    conv_out = vgg16_c(img_var)\n",
    "    print('VGG16 model:')\n",
    "    print(vgg16_c)\n",
    "\n",
    "    plt.ion() # remove blocking\n",
    "    plt.figure(figsize=(10,5))\n",
    "    vgg16_d = DNN()\n",
    "    done = False\n",
    "    while not done:\n",
    "        layer = input('Layer to view (0-30, -1 to exit): ')\n",
    "        try:\n",
    "            layer = int(layer)\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "        if layer < 0:\n",
    "            sys.exit(0)\n",
    "        activ_map = vgg16_c.feature_outputs[layer].data.numpy()\n",
    "        activ_map = activ_map.transpose(1,2,3,0)\n",
    "        activ_map_grid = vis_grid(activ_map)\n",
    "        vis_layer(activ_map_grid)\n",
    "\n",
    "        # only transpose convolve from Conv2d or ReLU layers\n",
    "        conv_layer = layer\n",
    "        if conv_layer not in conv_layer_indices:\n",
    "            conv_layer -= 1\n",
    "            if conv_layer not in conv_layer_indices:\n",
    "                continue\n",
    "\n",
    "        n_maps = activ_map.shape[0]\n",
    "\n",
    "        marker = None\n",
    "        while True:\n",
    "            choose_map = input('Select map?  (y/[n]): ') == 'y'\n",
    "            if marker != None:\n",
    "                marker.pop(0).remove()\n",
    "\n",
    "            if not choose_map:\n",
    "                break\n",
    "\n",
    "            _, map_x_dim, map_y_dim, _ = activ_map.shape\n",
    "            map_img_x_dim, map_img_y_dim, _ = activ_map_grid.shape\n",
    "            x_step = map_img_x_dim//(map_x_dim+1)\n",
    "\n",
    "            print('Click on an activation map to continue')\n",
    "            x_pos, y_pos = plt.ginput(1)[0]\n",
    "            x_index = x_pos // (map_x_dim+1)\n",
    "            y_index = y_pos // (map_y_dim+1)\n",
    "            map_idx = int(x_step*y_index + x_index)\n",
    "\n",
    "            if map_idx >= n_maps:\n",
    "                print('Invalid map selected')\n",
    "                continue\n",
    "\n",
    "            decon = vgg16_d(vgg16_c.feature_outputs[layer][0][map_idx][None,None,:,:], conv_layer, map_idx, vgg16_c.pool_indices)\n",
    "            img = decon_img(decon)\n",
    "            plt.subplot(121)\n",
    "            marker = plt.plot(x_pos, y_pos, marker='+', color='red')\n",
    "            plt.subplot(122)\n",
    "            plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
