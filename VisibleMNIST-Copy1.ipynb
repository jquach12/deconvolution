{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary packages.# Load  \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image\n",
    "import progressbar\n",
    "to_pil=ToPILImage()\n",
    "%matplotlib inline\n",
    "def load_image(path):\n",
    "    image = Image.open(path) #convert LA is greyscale\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Image loaded successfully\")\n",
    "    plt.axis(\"off\")\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    preprocess = transforms.Compose([\n",
    "    transforms.Scale(28),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "    image = Variable(preprocess(image).unsqueeze_(0), requires_grad=True)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU backward hook.\n",
    "\n",
    "def relu_backward_deconv_hook(module,grad_in,grad_out):\n",
    "     if isinstance(module, nn.ReLU):\n",
    "        return (torch.clamp(grad_out[0], min=0.0),)\n",
    "def Deconvolution(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,nn.ReLU):\n",
    "            m.register_backward_hook(relu_backward_deconv_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 1\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_sz,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "classes = [i for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c2c1198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTConvNet(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/pytorch/tutorials/blob/master/beginner_source/former_torchies/nn_tutorial.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "nn package\n",
    "==========\n",
    "We’ve redesigned the nn package, so that it’s fully integrated with\n",
    "autograd. Let's review the changes.\n",
    "**Replace containers with autograd:**\n",
    "    You no longer have to use Containers like ``ConcatTable``, or modules like\n",
    "    ``CAddTable``, or use and debug with nngraph. We will seamlessly use\n",
    "    autograd to define our neural networks. For example,\n",
    "    * ``output = nn.CAddTable():forward({input1, input2})`` simply becomes\n",
    "      ``output = input1 + input2``\n",
    "    * ``output = nn.MulConstant(0.5):forward(input)`` simply becomes\n",
    "      ``output = input * 0.5``\n",
    "**State is no longer held in the module, but in the network graph:**\n",
    "    Using recurrent networks should be simpler because of this reason. If\n",
    "    you want to create a recurrent network, simply use the same Linear layer\n",
    "    multiple times, without having to think about sharing weights.\n",
    "    .. figure:: /_static/img/torch-nn-vs-pytorch-nn.png\n",
    "       :alt: torch-nn-vs-pytorch-nn\n",
    "       torch-nn-vs-pytorch-nn\n",
    "**Simplified debugging:**\n",
    "    Debugging is intuitive using Python’s pdb debugger, and **the debugger\n",
    "    and stack traces stop at exactly where an error occurred.** What you see\n",
    "    is what you get.\n",
    "Example 1: ConvNet\n",
    "------------------\n",
    "Let’s see how to create a small ConvNet.\n",
    "All of your networks are derived from the base class ``nn.Module``:\n",
    "-  In the constructor, you declare all the layers you want to use.\n",
    "-  In the forward function, you define how your model is going to be\n",
    "   run, from input to output\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MNISTConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # this is the place where you instantiate all your modules\n",
    "        # you can later access them using the same names you've given them in\n",
    "        # here\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)  \n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(320, 50) #320\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    # it's the forward function that defines the network structure\n",
    "    # we're accepting only a single input in here, but if you want,\n",
    "    # feel free to use more\n",
    "    def forward(self, input):\n",
    "        x = self.pool1(F.relu(self.conv1(input)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # in your model definition you can go full crazy and use arbitrary\n",
    "        # python code to define your model structure\n",
    "        # all these are perfectly legal, and will be handled correctly\n",
    "        # by autograd:\n",
    "        # if x.gt(0) > x.numel() / 2:\n",
    "        #      ...\n",
    "        #\n",
    "        # you can even do a loop and reuse the same module inside it\n",
    "        # modules no longer hold ephemeral state, so you can use them\n",
    "        # multiple times during your forward pass\n",
    "        # while x.norm(2) < 10:\n",
    "        #    x = self.conv1(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "###############################################################\n",
    "# Let's use the defined ConvNet now.\n",
    "# You create an instance of the class first.\n",
    "\n",
    "\n",
    "net = torch.load('/Users/silver/Desktop/ucla_medical_imaging_informatics/dl4health_notebooks/deconvolution/vmnist.pt')\n",
    "print(net)\n",
    "\n",
    "#loading weights doesnt work for some reason\n",
    "#net.load_state_dict(torch.load('/Users/silver/Desktop/ucla_medical_imaging_informatics/dl4health_notebooks/deconvolution/vmnist.pt'))\n",
    "########################################################################\n",
    "# .. note::\n",
    "#\n",
    "#     ``torch.nn`` only supports mini-batches The entire ``torch.nn``\n",
    "#     package only supports inputs that are a mini-batch of samples, and not\n",
    "#     a single sample.\n",
    "#\n",
    "#     For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "#     ``nSamples x nChannels x Height x Width``.\n",
    "#\n",
    "#     If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "#     a fake batch dimension.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 3\n",
    "losses = []\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        losses.append(running_loss)\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PRINTS FOR DAYS\n",
    "\n",
    "\n",
    "# # #######################################################################\n",
    "# # The output of the ConvNet ``out`` is a ``Tensor``. We compute the loss\n",
    "# # using that, and that results in ``err`` which is also a ``Tensor``.\n",
    "# # Calling ``.backward`` on ``err`` hence will propagate gradients all the\n",
    "# # way through the ConvNet to it’s weights\n",
    "\n",
    "# # Let's access individual layer weights and gradients:\n",
    "\n",
    "# print(\"THIS IS CONV1 WEIGHT GRAD SIZE\")\n",
    "# print(net.conv1.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS CONV1 WEIGHT GRAD\")\n",
    "# print(net.conv1.weight.grad)\n",
    "\n",
    "\n",
    "# print(\"THIS IS POOL1 WEIGHT GRAD SIZE\")\n",
    "# print(net.pool1.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS POOL1 WEIGHT GRAD\")\n",
    "# print(net.pool1.weight.grad)\n",
    "\n",
    "\n",
    "# print(\"THIS IS CONV2 WEIGHT GRAD SIZE\")\n",
    "# print(net.conv2.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS CONV2 WEIGHT GRAD\")\n",
    "# print(net.conv2.weight.grad)\n",
    "\n",
    "# print(\"THIS IS FC1 WEIGHT GRAD SIZE\")\n",
    "# print(net.fc1.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS FC1 WEIGHT GRAD\")\n",
    "# print(net.fc1.weight.grad)\n",
    "\n",
    "# print(\"THIS IS FC2 WEIGHT GRAD SIZE\")\n",
    "# print(net.fc2.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS FC2 WEIGHT GRAD\")\n",
    "# print(net.fc2.weight.grad)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Losses over {} Epochs\".format(num_epochs))\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "torch.save(net, '/Users/silver/Desktop/ucla_medical_imaging_informatics/dl4health_notebooks/deconvolution/vmnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:      9\n",
      "None\n",
      "tensor([[  0.0000,   0.0000,   0.4381,   1.9179,   7.4563,   3.7402,\n",
      "           0.0000,   7.2065,   0.0000,  18.7825]])\n",
      "2.182135904149618e-05\n",
      "THIS IS GRAD\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c2c12b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(1)))\n",
    "\n",
    "#suppose i am interested in first layer\n",
    "print(net.conv1.weight.grad)\n",
    "\n",
    "outputs = net(images)#forward pass\n",
    "\n",
    "print(outputs)\n",
    "loss = criterion(outputs, labels)\n",
    "print(loss.item())\n",
    "print(\"THIS IS GRAD\")\n",
    "print(net.conv1.weight.data.grad)\n",
    "#print(net.conv1.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:      9\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deconvolution of image given the layer and number of filters to visualize.\n",
    "\n",
    "def deconvolution(image,layer=0,num_filters = 0):\n",
    "    \n",
    "    # Get the submodel\n",
    "    #vgg16_submodel=submodel(vgg16,last_layer)\n",
    "\n",
    "    jq_m = copy.deepcopy(net)\n",
    "    #jq_m = jq_m.load_state_dict(torch.load('/Users/silver/Desktop/net1_weight100ep.pt'))\n",
    "    #someLayer = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    Deconvolution(jq_m)\n",
    "    \n",
    "    #Forward pass.\n",
    "    #output=vgg16_submodel(image)\n",
    "    output = jq_m(image)\n",
    "    print(net.conv1.weight.data.grad)\n",
    "    print(jq_m.conv1.weight.grad)\n",
    "    \n",
    "    oldGrad = copy.deepcopy(jq_m.conv1.weight.grad)\n",
    "    print(oldGrad)\n",
    "    #Zero out the existing gradient buffers.\n",
    "    jq_m.zero_grad()\n",
    "\n",
    "    \n",
    "    print(\"THIS IS CONV1 WEIGHT AFTER ZERO GRAD\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     someTensor = torch.Tensor(1,5,5,10)\n",
    "#     print(someTensor)\n",
    "    \n",
    "#     print(jq_m)\n",
    "#     print(jq_m.conv1)\n",
    "#     print(jq_m.conv1.weight)\n",
    "#     print(jq_m.conv1.weight.grad)\n",
    "    \n",
    "    counter = 0\n",
    "    jq_m.conv1.weight.grad = torch.zeros(jq_m.conv1.weight.size())\n",
    "    jq_m.conv1.weight.grad[9] = torch.ones(5,5)\n",
    "            \n",
    "    print(jq_m.conv1.weight.grad)\n",
    "    gradients=[]\n",
    "    #bar=progressbar.ProgressBar(max_value=num_filters)\n",
    "\n",
    "    for filter_index in range(num_filters):\n",
    "\n",
    "        #Progress indicator\n",
    "        #bar.update(filter_index)\n",
    "\n",
    "        #Calculate the loss.\n",
    "        #loss=torch.max(output[0][filter_index])\n",
    "        #loss = torch.max(output[0])\n",
    "        loss = criterion(outputs,labels)\n",
    "        print(loss)\n",
    "        #Backward Pass\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "#         #Visualize the gradient image\n",
    "#         image.grad.data=image.grad.data-image.grad.data.min()\n",
    "#         image.grad.data/=(image.grad.data.max()-image.grad.data.min())\n",
    "#         grads=image.grad.data.squeeze(0)\n",
    "#         grads.transpose_(0,1) #don't use permute.\n",
    "#         grads.transpose_(1,2)\n",
    "#         gradients.append(grads)\n",
    "\n",
    "#     fig=plt.figure(figsize=(30,30),facecolor='black')    \n",
    "#     for i in range(int(np.sqrt(num_filters))*int(np.sqrt(num_filters))):\n",
    "#         a=fig.add_subplot(np.sqrt(num_filters),np.sqrt(num_filters), i+1)\n",
    "#         a.imshow(gradients[i])\n",
    "#         a.axis('off')\n",
    "#         plt.subplots_adjust(hspace=0.01,wspace=0.01,left=0.01,bottom=0.01)\n",
    "   # plt.savefig('/Users/silver/Desktop/deconvs/Deconvolution.jpg',facecolor=fig.get_facecolor(),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silver/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124990a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deconvolution visualization of a cat.\n",
    "\n",
    "cat=load_image(\"/Users/silver/Desktop/ucla_medical_imaging_informatics/dl4health_notebooks/deconvolution/9.png\")\n",
    "cat=preprocess_image(cat)\n",
    "\n",
    "#print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "THIS IS CONV1 WEIGHT AFTER ZERO GRAD\n",
      "tensor([[[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.]]]])\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n",
      "tensor(1.00000e-05 *\n",
      "       2.1821)\n"
     ]
    }
   ],
   "source": [
    "deconvolution(cat,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
