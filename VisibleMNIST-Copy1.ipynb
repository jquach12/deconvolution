{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary packages.# Load  \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image\n",
    "import progressbar\n",
    "to_pil=ToPILImage()\n",
    "%matplotlib inline\n",
    "def load_image(path):\n",
    "    image = Image.open(path) #convert LA is greyscale\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Image loaded successfully\")\n",
    "    plt.axis(\"off\")\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    preprocess = transforms.Compose([\n",
    "    transforms.Scale(28),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "    image = Variable(preprocess(image).unsqueeze_(0), requires_grad=True)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU backward hook.\n",
    "\n",
    "def relu_backward_deconv_hook(module,grad_in,grad_out):\n",
    "     if isinstance(module, nn.ReLU):\n",
    "        return (torch.clamp(grad_out[0], min=0.0),)\n",
    "def Deconvolution(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,nn.ReLU):\n",
    "            m.register_backward_hook(relu_backward_deconv_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 1\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_sz,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "classes = [i for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d9cf2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTConvNet(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/pytorch/tutorials/blob/master/beginner_source/former_torchies/nn_tutorial.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "nn package\n",
    "==========\n",
    "We’ve redesigned the nn package, so that it’s fully integrated with\n",
    "autograd. Let's review the changes.\n",
    "**Replace containers with autograd:**\n",
    "    You no longer have to use Containers like ``ConcatTable``, or modules like\n",
    "    ``CAddTable``, or use and debug with nngraph. We will seamlessly use\n",
    "    autograd to define our neural networks. For example,\n",
    "    * ``output = nn.CAddTable():forward({input1, input2})`` simply becomes\n",
    "      ``output = input1 + input2``\n",
    "    * ``output = nn.MulConstant(0.5):forward(input)`` simply becomes\n",
    "      ``output = input * 0.5``\n",
    "**State is no longer held in the module, but in the network graph:**\n",
    "    Using recurrent networks should be simpler because of this reason. If\n",
    "    you want to create a recurrent network, simply use the same Linear layer\n",
    "    multiple times, without having to think about sharing weights.\n",
    "    .. figure:: /_static/img/torch-nn-vs-pytorch-nn.png\n",
    "       :alt: torch-nn-vs-pytorch-nn\n",
    "       torch-nn-vs-pytorch-nn\n",
    "**Simplified debugging:**\n",
    "    Debugging is intuitive using Python’s pdb debugger, and **the debugger\n",
    "    and stack traces stop at exactly where an error occurred.** What you see\n",
    "    is what you get.\n",
    "Example 1: ConvNet\n",
    "------------------\n",
    "Let’s see how to create a small ConvNet.\n",
    "All of your networks are derived from the base class ``nn.Module``:\n",
    "-  In the constructor, you declare all the layers you want to use.\n",
    "-  In the forward function, you define how your model is going to be\n",
    "   run, from input to output\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MNISTConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # this is the place where you instantiate all your modules\n",
    "        # you can later access them using the same names you've given them in\n",
    "        # here\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)  \n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(320, 50) #320\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    # it's the forward function that defines the network structure\n",
    "    # we're accepting only a single input in here, but if you want,\n",
    "    # feel free to use more\n",
    "    def forward(self, input):\n",
    "        x = self.pool1(F.relu(self.conv1(input)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # in your model definition you can go full crazy and use arbitrary\n",
    "        # python code to define your model structure\n",
    "        # all these are perfectly legal, and will be handled correctly\n",
    "        # by autograd:\n",
    "        # if x.gt(0) > x.numel() / 2:\n",
    "        #      ...\n",
    "        #\n",
    "        # you can even do a loop and reuse the same module inside it\n",
    "        # modules no longer hold ephemeral state, so you can use them\n",
    "        # multiple times during your forward pass\n",
    "        # while x.norm(2) < 10:\n",
    "        #    x = self.conv1(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "###############################################################\n",
    "# Let's use the defined ConvNet now.\n",
    "# You create an instance of the class first.\n",
    "\n",
    "\n",
    "net = torch.load('/Users/silver/Desktop/ucla_medical_imaging_informatics/dl4health_notebooks/deconvolution/vmnist.pt')\n",
    "print(net)\n",
    "\n",
    "#loading weights doesnt work for some reason\n",
    "#net.load_state_dict(torch.load('/Users/silver/Desktop/ucla_medical_imaging_informatics/dl4health_notebooks/deconvolution/vmnist.pt'))\n",
    "########################################################################\n",
    "# .. note::\n",
    "#\n",
    "#     ``torch.nn`` only supports mini-batches The entire ``torch.nn``\n",
    "#     package only supports inputs that are a mini-batch of samples, and not\n",
    "#     a single sample.\n",
    "#\n",
    "#     For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "#     ``nSamples x nChannels x Height x Width``.\n",
    "#\n",
    "#     If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "#     a fake batch dimension.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 3\n",
    "losses = []\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        losses.append(running_loss)\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRINTS FOR DAYS\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# The output of the ConvNet ``out`` is a ``Tensor``. We compute the loss\n",
    "# using that, and that results in ``err`` which is also a ``Tensor``.\n",
    "# Calling ``.backward`` on ``err`` hence will propagate gradients all the\n",
    "# way through the ConvNet to it’s weights\n",
    "#\n",
    "# Let's access individual layer weights and gradients:\n",
    "\n",
    "# print(\"THIS IS CONV1 WEIGHT GRAD SIZE\")\n",
    "# print(net.conv1.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS CONV1 WEIGHT GRAD\")\n",
    "# print(net.conv1.weight.grad)\n",
    "\n",
    "#'MaxPool2d' object has no attribute 'weight'\n",
    "# print(\"THIS IS POOL1 WEIGHT GRAD SIZE\")\n",
    "# print(net.pool1.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS POOL1 WEIGHT GRAD\")\n",
    "# print(net.pool1.weight.grad)\n",
    "\n",
    "\n",
    "# print(\"THIS IS CONV2 WEIGHT GRAD SIZE\")\n",
    "# print(net.conv2.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS CONV2 WEIGHT GRAD\")\n",
    "# print(net.conv2.weight.grad)\n",
    "\n",
    "# print(\"THIS IS FC1 WEIGHT GRAD SIZE\")\n",
    "# print(net.fc1.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS FC1 WEIGHT GRAD\")\n",
    "# print(net.fc1.weight.grad)\n",
    "\n",
    "# print(\"THIS IS FC2 WEIGHT GRAD SIZE\")\n",
    "# print(net.fc2.weight.grad.size())\n",
    "\n",
    "# print(\"THIS IS FC2 WEIGHT GRAD\")\n",
    "# print(net.fc2.weight.grad)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Losses over {} Epochs\".format(num_epochs))\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "torch.save(net, '/Users/silver/Desktop/ucla_medical_imaging_informatics/dl4health_notebooks/deconvolution/vmnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:      2\n",
      "None\n",
      "tensor([[  3.2600,   3.1234,  12.6538,   3.0602,   2.0582,   0.0000,\n",
      "           0.7193,   1.0857,   0.0000,   0.0000]])\n",
      "0.0002746160316746682\n",
      "THIS IS GRAD\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x11d45acf8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/silver/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 349, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/silver/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 328, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/Users/silver/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/silver/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 86, in rebuild_storage_filename\n",
      "    storage = cls._new_shared_filename(manager, handle, size)\n",
      "RuntimeError: Interrupted system call at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/torch/lib/libshm/core.cpp:125\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADi9JREFUeJzt3X+MVXV6x/HPwwBGBQNmBSYCsiXG1Ki4zcSULDY2KyvIxpHEVfyL2qaziZh0tTE1+seSNKukutv2D0PCZskCYWVJBCHYuLsZS4FI0PFHUMAFQ6YsZYQaNgJCRPDpH3NoRpzzPXfuPfeeOz7vV0Luj+eec57c8Jlz7/2ec77m7gIQz5iqGwBQDcIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCosa3cmJlxOCHQZO5utbyuoT2/mS0wsz+Y2Udm9lQj6wLQWlbvsf1m1iHpoKT5ko5KekvSw+6+P7EMe36gyVqx579D0kfuftjdz0vaIKm7gfUBaKFGwn+9pD8OeXw0e+4rzKzHzPrMrK+BbQEoWSM/+A330eJrH+vdfZWkVRIf+4F20sie/6ikGUMeT5d0rLF2ALRKI+F/S9KNZvZtMxsvaYmkreW0BaDZ6v7Y7+4XzOwxSb+V1CFptbvvK60zAE1V91BfXRvjOz/QdC05yAfA6EX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFAtnaIbzdHR0ZFbmzZtWnLZ5cuXJ+s33XRTsj5v3rxkfe3atbm13t7e5LLr1q1L1tEY9vxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFRDs/SaWb+k05IuSrrg7l0Fr2eW3jpMmjQpWd+5c2du7eabby67ndIcPnw4WZ8/f36y3t/fX2I33xy1ztJbxkE+f+3un5SwHgAtxMd+IKhGw++Sfmdmb5tZTxkNAWiNRj/2f9fdj5nZFEm/N7MP3X3H0BdkfxT4wwC0mYb2/O5+LLs9IWmzpDuGec0qd+8q+jEQQGvVHX4zu9rMJl66L+n7kj4oqzEAzdXIx/6pkjab2aX1/NrdXyulKwBN19A4/4g3xjh/XXp60j+ZrFy5smnbPnfuXLJ+5ZVXNm3br776arK+ePHiZP3ixYtltjNq1DrOz1AfEBThB4Ii/EBQhB8IivADQRF+ICgu3d0GxoxJ/w2eM2dOsj4wMJBb27RpU3LZAwcOJOt9fX3JetFw2+OPP55bGz9+fHLZRYsWJetFlxXfv39/sh4de34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIpTettA0WmxM2bMSNYPHjxYZjul2r59e27tzjvvbGjdRdOD7969u6H1j1ac0gsgifADQRF+ICjCDwRF+IGgCD8QFOEHguJ8/jZQdHnsRsbxi86Zv+GGG+pedy2aeWnvot6jjvPXij0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVOM5vZqsl/UDSCXe/JXvuWkm/kTRLUr+kB939T81rEym33XZbbm3FihXJZe+5556y22mZvXv3Vt3CqFbLnv9XkhZc9txTknrd/UZJvdljAKNIYfjdfYekk5c93S1pTXZ/jaT7S+4LQJPV+51/qrsPSFJ2O6W8lgC0QtOP7TezHkk9zd4OgJGpd89/3Mw6JSm7PZH3Qndf5e5d7t5V57YANEG94d8qaWl2f6mkLeW0A6BVCsNvZi9J2i3pJjM7amZ/J2mFpPlmdkjS/OwxgFGk8Du/uz+cU/peyb0gx7Jly5L1J598MrdWdM3/dvbhhx8m60XXQUAaR/gBQRF+ICjCDwRF+IGgCD8QFOEHgmKK7jZw6623Juvbtm1L1qdPn173tosuCz5lSvq0jUmTJtW97VdeeSVZf+SRR5L1U6dO1b3tbzKm6AaQRPiBoAg/EBThB4Ii/EBQhB8IivADQTHO3wJjx6bPnN64cWOy3t3dXfe2z549m6wvXLgwWT99+nSyvnjx4mR9165dubU333wzuSzj+PVhnB9AEuEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4fwt0daUnK9qzZ09D60+N5T/66KPJZdetW9fQttF+GOcHkET4gaAIPxAU4QeCIvxAUIQfCIrwA0EVTtFtZqsl/UDSCXe/JXtuuaS/l/S/2cuedvf/aFaT7a6joyNZf+aZZ5q6/e3bt+fWGMdHnlr2/L+StGCY5//V3W/P/oUNPjBaFYbf3XdIOtmCXgC0UCPf+R8zs71mttrMJpfWEYCWqDf8KyXNlnS7pAFJP8t7oZn1mFmfmfXVuS0ATVBX+N39uLtfdPcvJf1C0h2J165y9y53T5/dAqCl6gq/mXUOebhY0gfltAOgVWoZ6ntJ0l2SvmVmRyX9RNJdZna7JJfUL+lHTewRQBNwPn8JqjxfX5IWLVqUW9uxY0dD2547d26yPm3atLrX/fHHHyfru3fvTtbvvvvuZH3ixIm5tddeey257Llz55L1dsb5/ACSCD8QFOEHgiL8QFCEHwiK8ANBFY7zo9gDDzzQ1PWPGzcuWV+yZElu7cUXX2xo2zNnzkzWJ0yYUPe6z5w5k6wfOXIkWZ89e3ayfsUVV+TWDh06lFz2+eefT9bXrl2brH/xxRfJejtgzw8ERfiBoAg/EBThB4Ii/EBQhB8IivADQXFKb43GjMn/O7lly5bksvfee2/Z7aBiGzZsSNZfeOGFZP3dd98ts52v4JReAEmEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4/w1Gj9+fG5tNF/muVFFl99+7733cmsLFgw3+fM3w6lTp5L16667Lrd24cKFhrbNOD+AJMIPBEX4gaAIPxAU4QeCIvxAUIQfCKrwuv1mNkPSWknTJH0paZW7/7uZXSvpN5JmSeqX9KC7/6l5rVbriSeeqLqFXL29vbm1FStWJJfdt29fQ9s+f/58sv7ZZ5/l1iZPntzQtovMmTMnt3bfffcll126dGmyftVVVyXr11xzTbK+fv363NpDDz2UXLYstez5L0j6R3f/c0l/KWmZmd0s6SlJve5+o6Te7DGAUaIw/O4+4O7vZPdPSzog6XpJ3ZLWZC9bI+n+ZjUJoHwj+s5vZrMkfUfSHklT3X1AGvwDIWlK2c0BaJ6a5+ozswmSXpb0Y3c/ZVbT4cMysx5JPfW1B6BZatrzm9k4DQZ/vbtvyp4+bmadWb1T0onhlnX3Ve7e5e5dZTQMoByF4bfBXfwvJR1w958PKW2VdOkn0aWS0pewBdBWCk/pNbN5knZKel+DQ32S9LQGv/dvlDRT0hFJP3T3kwXrGrWn9J49eza3lpoKuhavv/56sv7cc88l67t27cqtFQ3FYXibN29O1ouGChvR0dHR0PK1ntJb+J3f3XdJylvZ90bSFID2wRF+QFCEHwiK8ANBEX4gKMIPBEX4gaC4dHeNuru7c2ubNm3KrUnSyZPJwx/U2dmZrDd6KWeM3Nix6VHwouMAGpmWvVXj/Oz5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAoxvlLsHDhwmT9008/TdbfeOONMttBC8ydOzdZL5p+/Nlnn82tff7553X1dAnj/ACSCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMb5gW8YxvkBJBF+ICjCDwRF+IGgCD8QFOEHgiL8QFCF4TezGWb2n2Z2wMz2mdk/ZM8vN7P/MbP3sn/1X6gcQMsVHuRjZp2SOt39HTObKOltSfdLelDSGXd/oeaNcZAP0HS1HuSTnpZkcEUDkgay+6fN7ICk6xtrD0DVRvSd38xmSfqOpD3ZU4+Z2V4zW21mk3OW6TGzPjPra6hTAKWq+dh+M5sg6b8k/dTdN5nZVEmfSHJJ/6zBrwZ/W7AOPvYDTVbrx/6awm9m4yRtk/Rbd//5MPVZkra5+y0F6yH8QJOVdmKPmZmkX0o6MDT42Q+BlyyW9MFImwRQnVp+7Z8naaek9yV9mT39tKSHJd2uwY/9/ZJ+lP04mFoXe36gyUr92F8Wwg80H+fzA0gi/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFV4Ac+SfSLpv4c8/lb2XDtq197atS+J3upVZm831PrClp7P/7WNm/W5e1dlDSS0a2/t2pdEb/Wqqjc+9gNBEX4gqKrDv6ri7ae0a2/t2pdEb/WqpLdKv/MDqE7Ve34AFakk/Ga2wMz+YGYfmdlTVfSQx8z6zez9bObhSqcYy6ZBO2FmHwx57loz+72ZHcpuh50mraLe2mLm5sTM0pW+d+0243XLP/abWYekg5LmSzoq6S1JD7v7/pY2ksPM+iV1uXvlY8Jm9leSzkhae2k2JDP7F0kn3X1F9odzsrv/U5v0tlwjnLm5Sb3lzSz9N6rwvStzxusyVLHnv0PSR+5+2N3PS9ogqbuCPtqeu++QdPKyp7slrcnur9Hgf56Wy+mtLbj7gLu/k90/LenSzNKVvneJvipRRfivl/THIY+Pqr2m/HZJvzOzt82sp+pmhjH10sxI2e2Uivu5XOHMza102czSbfPe1TPjddmqCP9ws4m005DDd939LyQtlLQs+3iL2qyUNFuD07gNSPpZlc1kM0u/LOnH7n6qyl6GGqavSt63KsJ/VNKMIY+nSzpWQR/Dcvdj2e0JSZs1+DWlnRy/NElqdnui4n7+n7sfd/eL7v6lpF+owvcum1n6ZUnr3X1T9nTl791wfVX1vlUR/rck3Whm3zaz8ZKWSNpaQR9fY2ZXZz/EyMyulvR9td/sw1slLc3uL5W0pcJevqJdZm7Om1laFb937TbjdSUH+WRDGf8mqUPSanf/acubGIaZ/ZkG9/bS4BmPv66yNzN7SdJdGjzr67ikn0h6RdJGSTMlHZH0Q3dv+Q9vOb3dpRHO3Nyk3vJmlt6jCt+7Mme8LqUfjvADYuIIPyAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQf0ff9Ncaq65TxgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d72cba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(1)))\n",
    "\n",
    "#suppose i am interested in first layer\n",
    "print(net.conv1.weight.grad)\n",
    "\n",
    "outputs = net(images)#forward pass\n",
    "\n",
    "print(outputs)\n",
    "loss = criterion(outputs, labels)\n",
    "print(loss.item())\n",
    "print(\"THIS IS GRAD\")\n",
    "print(net.conv1.weight.data.grad)\n",
    "#print(net.conv1.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:      2\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deconvolution of image given the layer and number of filters to visualize.\n",
    "\n",
    "def deconvolution(image,layer=1,num_filters=10):\n",
    "    \n",
    "    # Get the submodel\n",
    "    #vgg16_submodel=submodel(vgg16,last_layer)\n",
    "\n",
    "    jq_m = copy.deepcopy(net)\n",
    "    #jq_m = jq_m.load_state_dict(torch.load('/Users/silver/Desktop/net1_weight100ep.pt'))\n",
    "    #someLayer = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    Deconvolution(jq_m)\n",
    "    \n",
    "    #Forward pass.\n",
    "    #output=vgg16_submodel(image)\n",
    "    output = jq_m(image)\n",
    "    print(net.conv1.weight.data.grad)\n",
    "    print(jq_m.conv1.weight.grad)\n",
    "\n",
    "    oldGrad = copy.deepcopy(jq_m.conv1.weight.grad)\n",
    "    print(oldGrad)\n",
    "    #Zero out the existing gradient buffers.\n",
    "    jq_m.zero_grad()\n",
    "\n",
    "    \n",
    "    print(\"THIS IS CONV1 WEIGHT AFTER ZERO GRAD\")\n",
    "    jq_m.conv1.weight.grad = torch.tensor(\n",
    "       [[[[ 1.6005,  1.5648,  1.6643, -0.1849, -1.6026],\n",
    "          [ 1.6005,  1.5945,  1.9234,  0.3486, -1.4137],\n",
    "          [ 1.6005,  1.6005,  2.0503,  0.5849, -1.2776],\n",
    "          [ 1.6005,  1.6005,  1.8965,  0.5325, -1.2548],\n",
    "          [ 1.6005,  1.6005,  1.9156,  0.5937, -1.2325]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
    "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
    "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
    "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
    "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
    "\n",
    "\n",
    "        [[[ 0.5548,  0.7837, -0.1532, -1.0173, -0.9955],\n",
    "          [ 0.5436,  0.8051,  0.1504, -0.9617, -1.0242],\n",
    "          [ 0.4243,  0.6914,  0.0819, -0.8780, -1.0206],\n",
    "          [ 0.1706,  0.3512, -0.0218, -0.7445, -1.0315],\n",
    "          [-0.2296,  0.2412, -0.0513, -0.7082, -0.8649]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0897,  0.0302, -0.0589, -0.0886, -0.0114],\n",
    "          [ 0.0897,  0.0348, -0.0544, -0.0886, -0.0366],\n",
    "          [ 0.0897,  0.0805,  0.0137, -0.0358,  0.0137],\n",
    "          [ 0.0897,  0.0897,  0.0897,  0.0897,  0.0897],\n",
    "          [ 0.0897,  0.0897,  0.0897,  0.0897,  0.0897]]],\n",
    "\n",
    "\n",
    "        [[[ 0.3525,  0.3522,  0.1599, -0.0021,  0.2041],\n",
    "          [ 0.3525,  0.3525,  0.1604, -0.0030,  0.1040],\n",
    "          [ 0.3525,  0.3525,  0.0633, -0.2841, -0.1845],\n",
    "          [ 0.3525,  0.3525,  0.0716, -0.3134, -0.3292],\n",
    "          [ 0.3525,  0.3525,  0.1216, -0.2653, -0.3479]]],\n",
    "\n",
    "\n",
    "        [[[-0.6577,  0.5441,  0.9341,  0.9892,  0.9892],\n",
    "          [-0.9111,  0.3427,  0.9517,  0.9892,  0.9892],\n",
    "          [-0.9786,  0.2975,  0.9130,  0.9892,  0.9892],\n",
    "          [-0.9750,  0.0200,  0.8881,  0.9892,  0.9892],\n",
    "          [-0.9747, -0.1483,  0.8581,  0.9892,  0.9892]]],\n",
    "\n",
    "\n",
    "        [[[-0.1005, -0.0781,  0.1283, -0.1422, -0.1614],\n",
    "          [-0.3503, -0.1469,  0.0951, -0.1422, -0.1614],\n",
    "          [-0.1564,  0.5052,  0.5806, -0.0643, -0.1614],\n",
    "          [-0.3320,  0.1256,  0.6242, -0.0826, -0.1614],\n",
    "          [-0.0387,  0.1936,  0.4835,  0.1320, -0.1614]]],\n",
    "\n",
    "\n",
    "        [[[ 0.1130,  0.1130,  0.1130,  0.1130,  0.1130],\n",
    "          [ 0.1130,  0.1130,  0.1130,  0.1130,  0.1130],\n",
    "          [ 0.0088,  0.1130,  0.1493,  0.2467,  0.2125],\n",
    "          [-0.0264,  0.1130,  0.1187,  0.2197,  0.2387],\n",
    "          [-0.0264,  0.0310, -0.0963,  0.0468,  0.2361]]],\n",
    "\n",
    "\n",
    "        [[[ 0.7191, -0.1266, -0.6638, -0.4075,  0.4927],\n",
    "          [ 0.5207, -0.1568, -0.5680, -0.6509,  0.4543],\n",
    "          [ 0.4216, -0.2389, -0.6909, -0.6417,  0.3196],\n",
    "          [ 0.3812, -0.2348, -0.6282, -0.6303,  0.3826],\n",
    "          [ 0.3091,  0.0850, -0.5506, -0.6292,  0.0578]]],\n",
    "\n",
    "\n",
    "        [[[ 0.8913,  0.8913,  0.2999, -0.8473, -0.5750],\n",
    "          [ 0.8913,  0.8913,  0.4006, -1.0022, -0.8332],\n",
    "          [ 0.8913,  0.8913,  0.3872, -0.7735, -0.8729],\n",
    "          [ 0.8913,  0.8913,  0.6272, -0.7834, -0.9410],\n",
    "          [ 0.8913,  0.8913,  0.6858, -0.4014, -0.8410]]]])\n",
    "    #print(jq_m.conv1.weight.grad)\n",
    "    for i in jq_m.conv1.weight.grad:\n",
    "        print(i)\n",
    "    gradients=[]\n",
    "    #bar=progressbar.ProgressBar(max_value=num_filters)\n",
    "\n",
    "    for filter_index in range(num_filters):\n",
    "\n",
    "        #Progress indicator\n",
    "        #bar.update(filter_index)\n",
    "\n",
    "        #Calculate the loss.\n",
    "        #loss=torch.max(output[0][filter_index])\n",
    "        #loss = torch.max(output[0])\n",
    "        loss = criterion(outputs,labels)\n",
    "        print(loss)\n",
    "        #Backward Pass\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "#         #Visualize the gradient image\n",
    "#         image.grad.data=image.grad.data-image.grad.data.min()\n",
    "#         image.grad.data/=(image.grad.data.max()-image.grad.data.min())\n",
    "#         grads=image.grad.data.squeeze(0)\n",
    "#         grads.transpose_(0,1) #don't use permute.\n",
    "#         grads.transpose_(1,2)\n",
    "#         gradients.append(grads)\n",
    "\n",
    "#     fig=plt.figure(figsize=(30,30),facecolor='black')    \n",
    "#     for i in range(int(np.sqrt(num_filters))*int(np.sqrt(num_filters))):\n",
    "#         a=fig.add_subplot(np.sqrt(num_filters),np.sqrt(num_filters), i+1)\n",
    "#         a.imshow(gradients[i])\n",
    "#         a.axis('off')\n",
    "#         plt.subplots_adjust(hspace=0.01,wspace=0.01,left=0.01,bottom=0.01)\n",
    "   # plt.savefig('/Users/silver/Desktop/deconvs/Deconvolution.jpg',facecolor=fig.get_facecolor(),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silver/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1190acc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deconvolution visualization of a cat.\n",
    "\n",
    "cat=load_image(\"/Users/silver/Desktop/ucla_medical_imaging_informatics/dl4health_notebooks/deconvolution/9.png\")\n",
    "cat=preprocess_image(cat)\n",
    "\n",
    "#print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "THIS IS CONV1 WEIGHT AFTER ZERO GRAD\n",
      "tensor([[[ 1.6005,  1.5648,  1.6643, -0.1849, -1.6026],\n",
      "         [ 1.6005,  1.5945,  1.9234,  0.3486, -1.4137],\n",
      "         [ 1.6005,  1.6005,  2.0503,  0.5849, -1.2776],\n",
      "         [ 1.6005,  1.6005,  1.8965,  0.5325, -1.2548],\n",
      "         [ 1.6005,  1.6005,  1.9156,  0.5937, -1.2325]]])\n",
      "tensor([[[ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  0.,  0.]]])\n",
      "tensor([[[ 0.5548,  0.7837, -0.1532, -1.0173, -0.9955],\n",
      "         [ 0.5436,  0.8051,  0.1504, -0.9617, -1.0242],\n",
      "         [ 0.4243,  0.6914,  0.0819, -0.8780, -1.0206],\n",
      "         [ 0.1706,  0.3512, -0.0218, -0.7445, -1.0315],\n",
      "         [-0.2296,  0.2412, -0.0513, -0.7082, -0.8649]]])\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 8.9700,  3.0200, -5.8900, -8.8600, -1.1400],\n",
      "         [ 8.9700,  3.4800, -5.4400, -8.8600, -3.6600],\n",
      "         [ 8.9700,  8.0500,  1.3700, -3.5800,  1.3700],\n",
      "         [ 8.9700,  8.9700,  8.9700,  8.9700,  8.9700],\n",
      "         [ 8.9700,  8.9700,  8.9700,  8.9700,  8.9700]]])\n",
      "tensor([[[ 0.3525,  0.3522,  0.1599, -0.0021,  0.2041],\n",
      "         [ 0.3525,  0.3525,  0.1604, -0.0030,  0.1040],\n",
      "         [ 0.3525,  0.3525,  0.0633, -0.2841, -0.1845],\n",
      "         [ 0.3525,  0.3525,  0.0716, -0.3134, -0.3292],\n",
      "         [ 0.3525,  0.3525,  0.1216, -0.2653, -0.3479]]])\n",
      "tensor([[[-0.6577,  0.5441,  0.9341,  0.9892,  0.9892],\n",
      "         [-0.9111,  0.3427,  0.9517,  0.9892,  0.9892],\n",
      "         [-0.9786,  0.2975,  0.9130,  0.9892,  0.9892],\n",
      "         [-0.9750,  0.0200,  0.8881,  0.9892,  0.9892],\n",
      "         [-0.9747, -0.1483,  0.8581,  0.9892,  0.9892]]])\n",
      "tensor([[[-0.1005, -0.0781,  0.1283, -0.1422, -0.1614],\n",
      "         [-0.3503, -0.1469,  0.0951, -0.1422, -0.1614],\n",
      "         [-0.1564,  0.5052,  0.5806, -0.0643, -0.1614],\n",
      "         [-0.3320,  0.1256,  0.6242, -0.0826, -0.1614],\n",
      "         [-0.0387,  0.1936,  0.4835,  0.1320, -0.1614]]])\n",
      "tensor([[[ 0.1130,  0.1130,  0.1130,  0.1130,  0.1130],\n",
      "         [ 0.1130,  0.1130,  0.1130,  0.1130,  0.1130],\n",
      "         [ 0.0088,  0.1130,  0.1493,  0.2467,  0.2125],\n",
      "         [-0.0264,  0.1130,  0.1187,  0.2197,  0.2387],\n",
      "         [-0.0264,  0.0310, -0.0963,  0.0468,  0.2361]]])\n",
      "tensor([[[ 0.7191, -0.1266, -0.6638, -0.4075,  0.4927],\n",
      "         [ 0.5207, -0.1568, -0.5680, -0.6509,  0.4543],\n",
      "         [ 0.4216, -0.2389, -0.6909, -0.6417,  0.3196],\n",
      "         [ 0.3812, -0.2348, -0.6282, -0.6303,  0.3826],\n",
      "         [ 0.3091,  0.0850, -0.5506, -0.6292,  0.0578]]])\n",
      "tensor([[[ 0.8913,  0.8913,  0.2999, -0.8473, -0.5750],\n",
      "         [ 0.8913,  0.8913,  0.4006, -1.0022, -0.8332],\n",
      "         [ 0.8913,  0.8913,  0.3872, -0.7735, -0.8729],\n",
      "         [ 0.8913,  0.8913,  0.6272, -0.7834, -0.9410],\n",
      "         [ 0.8913,  0.8913,  0.6858, -0.4014, -0.8410]]])\n",
      "tensor(1.00000e-04 *\n",
      "       2.7462)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-739e10e1be42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeconvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-78611c0464ba>\u001b[0m in \u001b[0;36mdeconvolution\u001b[0;34m(image, layer, num_filters)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m#Visualize the gradient image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m/=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "deconvolution(cat,1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
